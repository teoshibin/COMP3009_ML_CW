{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library stuff\n",
    "import warnings\n",
    "\n",
    "from numpy.lib.function_base import average\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import math\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "n_input = 12\n",
    "n_hidden1 = 24\n",
    "n_hidden2 = 12\n",
    "n_hidden3 = 6\n",
    "n_output = 2\n",
    "\n",
    "#Learning parameters\n",
    "learning_constant = 0.2 \n",
    "number_epochs = 20000\n",
    "batch_size = 1000\n",
    "\n",
    "#Defining the input and the output\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "#DEFINING WEIGHTS AND BIASES\n",
    "b1 = tf.Variable(tf.random_normal([n_hidden1]))\n",
    "b2 = tf.Variable(tf.random_normal([n_hidden2]))\n",
    "b3 = tf.Variable(tf.random_normal([n_hidden3]))\n",
    "b4 = tf.Variable(tf.random_normal([n_output]))\n",
    "w1 = tf.Variable(tf.random_normal([n_input, n_hidden1]))\n",
    "w2 = tf.Variable(tf.random_normal([n_hidden1, n_hidden2]))\n",
    "w3 = tf.Variable(tf.random_normal([n_hidden2, n_hidden3]))\n",
    "w4 = tf.Variable(tf.random_normal([n_hidden3, n_output]))\n",
    "\n",
    "def multilayer_perceptron(input_d):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(input_d, w1), b1)) #f(X * W1 + b1)\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w2), b2))\n",
    "    layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, w3), b3))\n",
    "    out_layer = tf.nn.softmax(tf.add(tf.matmul(layer_3, w4),b4))\n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadHeartFailureDataset():    #just get the data\n",
    "\n",
    "    path = os.path.join(\"datasets\",\"heart_failure_clinical_records_dataset.csv\")\n",
    "    data = np.genfromtxt(path, delimiter=\",\", names=True)\n",
    "\n",
    "    #splitting data and label out\n",
    "    data = data.view(np.float64).reshape((len(data), -1))\n",
    "    x_data = data[:, 0:-1]\n",
    "    y_data = data[:, -1]\n",
    "    y_data = y_data.astype('int32')\n",
    "    y_data = np.identity(2)[y_data] # one hot encoding\n",
    "\n",
    "    # print(x_data)\n",
    "    # print(y_data)\n",
    "    return x_data, y_data\n",
    "\n",
    "def minMaxNorm(data): #normalize the data\n",
    "    spacing = 5\n",
    "    pct_min = np.percentile(data, spacing, axis=0)\n",
    "    pct_max = np.percentile(data, 100 - spacing, axis=0)\n",
    "    norm_data = np.zeros(data.shape)\n",
    "    for i in range(len(data[0])):\n",
    "        norm_data[:, i] = (data[:,i] - pct_min[i]) / (pct_max[i] - pct_min[i])\n",
    "\n",
    "    norm_data[norm_data > 1] = 1\n",
    "    norm_data[norm_data < 0] = 0\n",
    "\n",
    "    return norm_data\n",
    "\n",
    "def splitLabel(data, number_of_labels):\n",
    "    number_of_columns = len(data[0])\n",
    "    \n",
    "    x_data = data[:, 0:-number_of_labels]\n",
    "    y_data = data[:, number_of_columns - number_of_labels : number_of_columns]\n",
    "    return x_data, y_data\n",
    "\n",
    "def mergeLabel(x_data, y_data):\n",
    "    return np.column_stack((x_data, y_data))\n",
    "\n",
    "def validationSplit(data, percentage): #split the data between train and test\n",
    "    portion = round(data.shape[0]*percentage)\n",
    "    train = data[0:portion]\n",
    "    test = data[portion:data.shape[0]]\n",
    "    return train, test\n",
    "\n",
    "def partitionIndex(instances_size, k): \n",
    "    partitionIndicies = np.zeros((k, 2))\n",
    "    itemsPerFold = round(instances_size / k)\n",
    "    for i in range(k): \n",
    "        if i == 0:\n",
    "            partitionIndicies[i, :] = [(i)*itemsPerFold, (i + 1)*itemsPerFold - 1]\n",
    "        else:\n",
    "            partitionIndicies[i, :] = [(i)*itemsPerFold, (i + 1)*itemsPerFold - 1]\n",
    "        \n",
    "    partitionIndicies[k - 1,1] = instances_size - 1\n",
    "    return partitionIndicies\n",
    "\n",
    "def splitPartition(data, range_indices, partition_selection):\n",
    "    range_list = np.arange(range_indices[partition_selection,0],range_indices[partition_selection,1] + 1, dtype = int)\n",
    "    is_test_mat = np.zeros((data.shape[0], 1), dtype = int)\n",
    "    \n",
    "    for i in range_list:\n",
    "        is_test_mat[i] = 1\n",
    "        \n",
    "    test_data = np.empty((0,data.shape[1]))\n",
    "    train_data = np.empty((0,data.shape[1]))\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        if(is_test_mat[i] == 1):\n",
    "            test_data = np.append(test_data, [data[i,:]], axis = 0)\n",
    "        else:\n",
    "            train_data = np.append(train_data, [data[i,:]], axis = 0)\n",
    "       \n",
    "    return test_data, train_data\n",
    "    \n",
    "def shuffleRow(data):\n",
    "    np.random.seed(2)\n",
    "    np.random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def f1Score(predicted, actual):\n",
    "    TP = tf.count_nonzero(predicted * actual)\n",
    "    TN = tf.count_nonzero((predicted - 1) * (actual - 1))\n",
    "    FP = tf.count_nonzero(predicted * (actual - 1))\n",
    "    FN = tf.count_nonzero((predicted - 1) * actual)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def oneHotEncoding(y_data):\n",
    "    y_data = np.identity(2)[y_data]\n",
    "    return y_data\n",
    "\n",
    "def unOneHotEncoding(y_data, axis):\n",
    "    y_data = np.argmax(y_data, axis)\n",
    "    return y_data\n",
    "\n",
    "def maxCountOccur(data):\n",
    "    new_list = [i for x in data for i in x]\n",
    "    max_val = max(new_list, key=lambda x:new_list.count(x)) \n",
    "    return max_val\n",
    "\n",
    "def findMean(data):\n",
    "    meanValue = np.mean(data)\n",
    "    return meanValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_network,labels=Y))\n",
    "#Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Load dataset\n",
    "x_data, y_data = loadHeartFailureDataset()\n",
    "x_data = minMaxNorm(x_data)\n",
    "\n",
    "number_of_labels = len(y_data[0]) #because of the output required 2\n",
    "# train_x = x_data\n",
    "# train_y = y_data\n",
    "\n",
    "data = mergeLabel(x_data, y_data)\n",
    "data = shuffleRow(data)\n",
    "#train_set, test_set = validationSplit(data, 0.5)\n",
    "#train_x, train_y = splitLabel(train_set, number_of_labels)\n",
    "#test_x, test_y = splitLabel(test_set, number_of_labels)\n",
    "# print(\"\\nTrain X:\\n\", train_x)\n",
    "# print(\"\\nTrain Y:\\n\", train_y)\n",
    "# print(\"\\nTest X:\\n\", test_x)\n",
    "# print(\"\\nTest Y:\\n\", test_y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    k = 10\n",
    "    inner_k = 5\n",
    "    \n",
    "    accuracies = np.zeros((1, k))\n",
    "    f1scores = np.zeros((1, k))\n",
    "    all_best_hyper_learning = np.zeros((k, inner_k))\n",
    "    epochs = 200\n",
    "    \n",
    "    losses = np.zeros((50,200), dtype = int)\n",
    "    incre = 0\n",
    "    \n",
    "    #setup outer partition\n",
    "    outer_range_indices = partitionIndex(data.shape[0], k)\n",
    "    \n",
    "    for i in range(k): \n",
    "        #create outer partition\n",
    "        outer_test_data, outer_train_data = splitPartition(data, outer_range_indices, i)\n",
    "        \n",
    "        outer_train_X, outer_train_Y = splitLabel(outer_train_data, number_of_labels)\n",
    "        outer_test_X, outer_test_Y = splitLabel(outer_test_data, number_of_labels)\n",
    "        \n",
    "     \n",
    "        #setup inner partition\n",
    "        inner_range_indices = partitionIndex(outer_train_data.shape[0], inner_k)\n",
    "       \n",
    "        for j in range(inner_k):\n",
    "            #create inner partition\n",
    "            inner_test_data, inner_train_data = splitPartition(outer_train_data, inner_range_indices, j)\n",
    "           \n",
    "            bestF1Score = 0\n",
    "            \n",
    "            inner_train_X, inner_train_Y = splitLabel(inner_train_data, number_of_labels)\n",
    "            inner_test_X, inner_test_Y = splitLabel(inner_test_data, number_of_labels)\n",
    "    \n",
    "            #grid search\n",
    "            for hyper_learning in np.arange(0.01,0.06,0.01):\n",
    "\n",
    "                #Create model\n",
    "                neural_network = multilayer_perceptron(X)\n",
    "\n",
    "                #Define loss and optimizer\n",
    "                loss_op = tf.reduce_mean(tf.math.squared_difference(neural_network,Y))\n",
    "                \n",
    "                optimizer = tf.train.GradientDescentOptimizer(hyper_learning).minimize(loss_op)\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    sess.run(optimizer, feed_dict={X: inner_train_X, Y: inner_train_Y})\n",
    "                    \n",
    "                    pred = (neural_network)\n",
    "                    mse_loss_obj = tf.keras.losses.MSE(pred,Y)\n",
    "                    loss = mse_loss_obj.eval({X: inner_train_X, Y: inner_train_Y})\n",
    "                    losses[incre][epoch] = np.mean(loss) \n",
    "                    \n",
    "                           \n",
    "                output = neural_network.eval({X: inner_test_X})\n",
    "                correct_prediction = tf.equal(tf.argmax(output,1),unOneHotEncoding(inner_test_Y,1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                actual = unOneHotEncoding(inner_test_Y,1)\n",
    "                predicted = tf.argmax(output,1)\n",
    "                f1 = f1Score(predicted,actual)\n",
    "                value_f1 = tf.keras.backend.get_value(f1)\n",
    "                \n",
    "                incre = incre + 1\n",
    "                \n",
    "                if value_f1 > bestF1Score:\n",
    "                    all_best_hyper_learning[i,j] = hyper_learning\n",
    "                    bestF1Score = value_f1\n",
    "                    \n",
    "            \n",
    "            print(\"\\t Inner: %d Testsize: %d Trainsize: %d Accuracy: %f F1Score: %f BestLearning: %f\"\n",
    "                        %(j, inner_test_data.shape[0], inner_train_data.shape[0], tf.keras.backend.get_value(accuracy), bestF1Score,\n",
    "                        all_best_hyper_learning[i,j]))\n",
    "                 \n",
    "        most_learning = maxCountOccur(all_best_hyper_learning[0:i + 1,:])\n",
    "       \n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(most_learning).minimize(loss_op) \n",
    "        for epoch in range(epochs):\n",
    "            sess.run(optimizer, feed_dict={X: outer_train_X, Y: outer_train_Y})\n",
    "        \n",
    "        output = neural_network.eval({X: outer_test_X})\n",
    "        correct_prediction = tf.equal(tf.argmax(output,1),unOneHotEncoding(outer_test_Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        accuracies[0][i] = tf.keras.backend.get_value(accuracy)\n",
    "        actual = unOneHotEncoding(outer_test_Y,1)\n",
    "        predicted = tf.argmax(output,1)\n",
    "        f1 = f1Score(predicted,actual)\n",
    "        f1scores[0][i] = tf.keras.backend.get_value(f1)\n",
    "        print(\"Outer: %d Testsize: %d Trainsize: %d Accuracy: %f F1Score: %f MostBestLearning: %f\"\n",
    "                        %(i, outer_test_X.shape[0], outer_train_X.shape[0], accuracies[0][i], f1scores[0][i],\n",
    "                        most_learning))\n",
    "\n",
    "    mean_accuracy = findMean(accuracies)\n",
    "    mean_f1score = findMean(f1scores)\n",
    "    best_hyper_learning = maxCountOccur(all_best_hyper_learning)\n",
    "    print(\"FinalAccuracy: %f FinalF1: %f FinalLearning: %f\"\n",
    "          %(mean_accuracy, mean_f1score, best_hyper_learning))\n",
    "    print(losses)\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c93ae1489fc19151af6bb7fee92a04837d2023b1c0e60c88e75afb439526336d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('mle_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
