{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import math\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os \n",
    "# import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Network parameters\n",
    "n_input = 12\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 10\n",
    "n_output = 2\n",
    "\n",
    "#Learning parameters\n",
    "learning_constant = 0.2\n",
    "number_epochs = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "#Defining the input and the output\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "#DEFINING WEIGHTS AND BIASES\n",
    "b1 = tf.Variable(tf.random_normal([n_hidden1]))\n",
    "b2 = tf.Variable(tf.random_normal([n_hidden2]))\n",
    "b3 = tf.Variable(tf.random_normal([n_output]))\n",
    "w1 = tf.Variable(tf.random_normal([n_input, n_hidden1]))\n",
    "w2 = tf.Variable(tf.random_normal([n_hidden1, n_hidden2]))\n",
    "w3 = tf.Variable(tf.random_normal([n_hidden2, n_output]))\n",
    "\n",
    "def multilayer_perceptron(input_d):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(input_d, w1), b1))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w2), b2))\n",
    "    out_layer = tf.add(tf.matmul(layer_2, w3),b3)\n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i:  0 \n",
      "5:  42.900000000000006 \n",
      "95:  82.0\n",
      "[75. 55. 65. 50. 65.]\n",
      "[0.82097187 0.30946292 0.56521739 0.18158568 0.56521739]\n",
      "\n",
      "i:  1 \n",
      "5:  0.0 \n",
      "95:  1.0\n",
      "[0. 0. 0. 1. 1.]\n",
      "[0. 0. 0. 1. 1.]\n",
      "\n",
      "i:  2 \n",
      "5:  59.0 \n",
      "95:  2262.9999999999995\n",
      "[ 582. 7861.  146.  111.  160.]\n",
      "[0.23729583 3.5399274  0.03947368 0.02359347 0.04582577]\n",
      "\n",
      "i:  3 \n",
      "5:  0.0 \n",
      "95:  1.0\n",
      "[0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.]\n",
      "\n",
      "i:  4 \n",
      "5:  20.0 \n",
      "95:  60.0\n",
      "[20. 38. 20. 20. 20.]\n",
      "[0.   0.45 0.   0.   0.  ]\n",
      "\n",
      "i:  5 \n",
      "5:  0.0 \n",
      "95:  1.0\n",
      "[1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0.]\n",
      "\n",
      "i:  6 \n",
      "5:  131800.0 \n",
      "95:  422499.9999999998\n",
      "[265000.   263358.03 162000.   210000.   327000.  ]\n",
      "[0.45820433 0.452556   0.10388717 0.26900585 0.67148263]\n",
      "\n",
      "i:  7 \n",
      "5:  0.7 \n",
      "95:  3.0\n",
      "[1.9 1.1 1.3 1.9 2.7]\n",
      "[0.52173913 0.17391304 0.26086957 0.52173913 0.86956522]\n",
      "\n",
      "i:  8 \n",
      "5:  130.0 \n",
      "95:  144.0\n",
      "[130. 136. 129. 137. 116.]\n",
      "[ 0.          0.42857143 -0.07142857  0.5        -1.        ]\n",
      "\n",
      "i:  9 \n",
      "5:  0.0 \n",
      "95:  1.0\n",
      "[1. 1. 1. 1. 0.]\n",
      "[1. 1. 1. 1. 0.]\n",
      "\n",
      "i:  10 \n",
      "5:  0.0 \n",
      "95:  1.0\n",
      "[0. 0. 1. 0. 0.]\n",
      "[0. 0. 1. 0. 0.]\n",
      "\n",
      "i:  11 \n",
      "5:  12.9 \n",
      "95:  250.0\n",
      "[4. 6. 7. 7. 8.]\n",
      "[-0.0375369  -0.02910164 -0.02488402 -0.02488402 -0.02066639]\n",
      "[[0.82097187 0.         0.23729583 0.         0.         1.\n",
      "  0.45820433 0.52173913 0.         1.         0.         0.        ]\n",
      " [0.30946292 0.         1.         0.         0.45       0.\n",
      "  0.452556   0.17391304 0.42857143 1.         0.         0.        ]\n",
      " [0.56521739 0.         0.03947368 0.         0.         0.\n",
      "  0.10388717 0.26086957 0.         1.         1.         0.        ]\n",
      " [0.18158568 1.         0.02359347 0.         0.         0.\n",
      "  0.26900585 0.52173913 0.5        1.         0.         0.        ]\n",
      " [0.56521739 1.         0.04582577 1.         0.         0.\n",
      "  0.67148263 0.86956522 0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def loadHeartFailureDataset():    \n",
    "\n",
    "    path = os.path.join(\"datasets\",\"heart_failure_clinical_records_dataset.csv\")\n",
    "    data = np.genfromtxt(path, delimiter=\",\", names=True)\n",
    "\n",
    "    data = data.view(np.float64).reshape((len(data), -1))\n",
    "    x_data = data[:, 0:-1]\n",
    "    y_data = data[:, -1]\n",
    "    y_data = y_data.astype('int32')\n",
    "    y_data = np.identity(2)[y_data] # one hot encoding\n",
    "\n",
    "    # print(x_data)\n",
    "    # print(y_data)\n",
    "    return x_data, y_data\n",
    "\n",
    "def splitLabel(data, number_of_labels):\n",
    "    number_of_columns = len(data[0])\n",
    "\n",
    "    x_data = data[:, 0:-number_of_labels]\n",
    "    y_data = data[:, number_of_columns - number_of_labels : number_of_columns]\n",
    "    return x_data, y_data\n",
    "\n",
    "def mergeLabel(x_data, y_data):\n",
    "    return np.column_stack((x_data, y_data))\n",
    "\n",
    "def validationSplit(data, percentage):\n",
    "    portion = round(len(data)*percentage)\n",
    "    train = data[0:portion]\n",
    "    test = data[portion:len(data)]\n",
    "    return train, test\n",
    "\n",
    "def minMaxNorm(data):\n",
    "    spacing = 5\n",
    "    pct_min = np.percentile(data, spacing, axis=0)\n",
    "    pct_max = np.percentile(data, 100 - spacing, axis=0)\n",
    "    norm_data = np.zeros(data.shape)\n",
    "    for i in range(len(data[0])):\n",
    "        norm_data[:, i] = (data[:,i] - pct_min[i]) / (pct_max[i] - pct_min[i])\n",
    "\n",
    "    norm_data[norm_data > 1] = 1\n",
    "    norm_data[norm_data < 0] = 0\n",
    "\n",
    "    return norm_data\n",
    "\n",
    "\n",
    "x_data, y_data = loadHeartFailureDataset()\n",
    "norm_x_data = minMaxNorm(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create model\n",
    "neural_network = multilayer_perceptron(X)\n",
    "\n",
    "#Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.math.squared_difference(neural_network,Y))\n",
    "#loss_op =\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=neural_network,labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_constant).minimize(loss_op)\n",
    "\n",
    "#Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "x_data, y_data = loadHeartFailureDataset()\n",
    "number_of_labels = len(y_data[0])\n",
    "\n",
    "data = mergeLabel(x_data, y_data)\n",
    "train_set, test_set = validationSplit(data, 0.8)\n",
    "train_x, train_y = splitLabel(train_set, number_of_labels)\n",
    "test_x, test_y = splitLabel(test_set, number_of_labels)\n",
    "\n",
    "# batch_x1=np.loadtxt('x1.txt')\n",
    "# batch_x2=np.loadtxt('x2.txt')\n",
    "# batch_y1=np.loadtxt('y1.txt')\n",
    "# batch_y2=np.loadtxt('y2.txt')\n",
    "\n",
    "# label=batch_y2#+1e-50-1e-50\n",
    "\n",
    "# batch_x=np.column_stack((batch_x1, batch_x2))\n",
    "# batch_y=np.column_stack((batch_y1, batch_y2))\n",
    "\n",
    "# batch_x_train=batch_x[:,0:599]\n",
    "# batch_y_train=batch_y[:,0:599]\n",
    "# batch_x_test=batch_x[:,600:1000]\n",
    "# batch_y_test=batch_y[:,600:1000]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #Training epoch\n",
    "    for epoch in range(number_epochs):\n",
    "        sess.run(optimizer, feed_dict={X: train_x, Y: train_y})\n",
    "        #Display the epoch\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", '%d' % (epoch))\n",
    "\n",
    "    # Test model\n",
    "    pred = (neural_network) # Apply softmax to logits\n",
    "    accuracy = tf.keras.losses.MSE(pred,Y)\n",
    "    print(\"Accuracy:\", accuracy.eval({X: train_x, Y: train_y}))\n",
    "    \n",
    "    #tf.keras.evaluate(pred,batch_x)\n",
    "    print(\"Prediction:\", pred.eval({X: train_x}))\n",
    "    output=neural_network.eval({X: train_x})\n",
    "    plt.plot(train_y[0:10], 'ro', output[0:10], 'bo')\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.show()\n",
    "\n",
    "    # estimated_class=tf.argmax(pred, 1)#+1e-50-1e-50\n",
    "    # correct_prediction1 = tf.equal(tf.argmax(pred, 1),label)\n",
    "    # accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, tf.float32))\n",
    "    \n",
    "    # print(accuracy1.eval({X: batch_x}))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55da5dce4dc80e48c7cd5a6f85911ea3c7910d9a74481d0cac693d01048402c7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('mle_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
